{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7952d5ab",
   "metadata": {},
   "source": [
    "# Homework 2 (HW2)\n",
    "\n",
    "By the end of this homework, we expect you to be able to:\n",
    "\n",
    "- Preprocess data and make it amenable to statistical analysis and machine learning models;\n",
    "- Train and test out-of-the-box machine learning models in `sklearn`;\n",
    "- Carry out simple multivariate regression analyses using `statsmodels`;\n",
    "- Use propensity score matching to estimate treatment effects;\n",
    "\n",
    "---\n",
    "\n",
    "## Important Dates\n",
    "\n",
    "- Homework release: Fri 15 Nov 2024\n",
    "- **Homework due**: Fri 29 Nov 2024, 23:59\n",
    "- Grade release: Mon 09 Dec 2024\n",
    "\n",
    "---\n",
    "\n",
    "##  Some rules\n",
    "\n",
    "1. You are allowed to use any built-in Python library that is included in the `requirements.txt` for this homework. If you use any additional library, this may complicate the grading process, and we reserve the right to penalize your grade for unnecessary complexity of the solution. All the questions can be solved with the libraries in `requirements.txt`.\n",
    "2. Please write all your comments in English, and use meaningful variable names in your code. Your repo should have a single notebook (plus the required data files) in the *master/main* branch. If there are multiple notebooks present, we will **not grade** anything.\n",
    "3. We will **not run your notebook for you**! Rather, we will grade it as is, which means that only the results contained in your evaluated code cells will be considered, and we will not see the results in unevaluated code cells. Thus, be sure to hand in a **fully-run and evaluated notebook**. In order to check whether everything looks as intended, you can check the rendered notebook on the GitHub website once you have pushed your solution there.\n",
    "4. In continuation to the previous point on additional library, interactive plots, such as those generated using `plotly`, should be **strictly avoided**!\n",
    "\n",
    "**A Note on using Language Models (LMs)**\n",
    "\n",
    "If you try hard enough, you will likely get away with cheating. Fortunately, our job is not to police, but rather to educate! So, please consider the following:\n",
    "\n",
    "Presumably, you are taking this course to learn something! LMs are not always right ([they often fail in silly ways](https://community.openai.com/t/why-9-11-is-larger-than-9-9-incredible/869824/4)). This course should prepare you to detect when they are wrong!\n",
    "Some of the TAs on this course literally published many works on detecting machine-generated text.\n",
    "\n",
    "Here LLM includes but not limited to chatbots like ChatGPT, coding assistants like Copilot. Do not even use them to prettify your code or correct English. If you are caught using LLMs, you will be reported to the instructor and subject to the consequences. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Grading \n",
    "- The homework has a total of 100 points, distributed as follows:\n",
    "    - Part 1: Data Preprocessing ( 20 points)\n",
    "    - Part 2: Linear Regression (30 points)\n",
    "    - Part 3: Supervised Learning (40 points)\n",
    "    - Part 4: Propensity Score Matching (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99beda",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "Within EPFL's master program, you are excited to start an internship as a data scientist.\n",
    "After rounds of interviews, you have been selected to work with the biggest car dealership in Switzerland !\n",
    "\n",
    "Your mentor at the company Tim, has explained to you that the company is interested in a pricing model for used cars. \n",
    "\n",
    "- Tim: \"We have a lot of used cars in our inventory, and we need to determine the price at which we should sell these cars. We have some ideas about the factors that influence the price of a used car, but so far we have just been using our experience and intuition to determine the price of a used car. Sometimes it works, but probably we can do better and a more data-driven approach would also help our new employees in the sales team as they have less experience.\"\n",
    "\n",
    "- You: \"That sounds like a great project! What kind of data do we have?\"\n",
    "\n",
    "- Tim: \"We sell all kinds of cars here, but maybe we can start with a specific brand and model. For example, the Toyota Corolla is the best-selling car worldwide in 2023, and we have a lot of data on it. We can start by analyzing the data on used Toyota Corolla cars. If it works well, we can extend the analysis to other brands.\"\n",
    "\n",
    "The dataset contains the following columns:\n",
    "\n",
    "- `Age`: Age of the car in months.\n",
    "- `Mileage`: Number of distance the car has been driven. (km or miles)\n",
    "- `FuelType`: Fuel type of the car (Petrol, Diesel, or CNG)\n",
    "- `HP`: Horsepower\n",
    "- `MetColor`: Is the color of the car metallic? (Yes=1, No=0)\n",
    "- `Automatic`: Is the car automatic? (Yes=1, No=0)\n",
    "- `CC`: Cylinder volume in cubic centimeters\n",
    "- `Doors`: Number of doors\n",
    "- `Weight`: Weight of the car in kilograms\n",
    "- `Price`: Price of the car in euros\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "The data is provided in the `data` folder and it contains the following 3 csv files:\n",
    "- `Task1-2.ToyotaCorolla-clean.csv` and `Task1-2.ToyotaCorolla-raw.csv` for Part 1 and Part 2\n",
    "- `Task3.ToyotaCorolla_sales_3months.csv` for Part 3\n",
    "- `Task4.ToyotaCorolla_discount_sales` for Part 4\n",
    "\n",
    "You should not use any other data source for this homework.\n",
    "\n",
    "## References:\n",
    "\n",
    "The data is based on the ToyotaCorolla dataset from the UCI Machine Learning Repository [here](https://archive.ics.uci.edu/ml/datasets/Toyota+Corolla).\n",
    "We have made some modifications to the original dataset, so please use the data provided in the `data` folder in the course repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a42aa7",
   "metadata": {},
   "source": [
    "## Task 1 (20 pts) - Get to know the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faafd098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "import sklearn\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a1d5a5",
   "metadata": {},
   "source": [
    "\n",
    "**1.1 (2 pts)**: Load the data from the file `Task1-2.ToyotaCorolla-raw.csv` into a pandas DataFrame. Display the first 5 rows of the DataFrame. Hint: A naive loading of the data will raise an error. You will need to figure out how to load the data correctly. (Hint: localise which row is causing the error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce30454b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413fa896",
   "metadata": {},
   "source": [
    "**1.2 (2 pts)**: Check if there are nan values in the Dataframe. If there are, try to find out which row is problematic and fix it. If you can't fix it, drop the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73dda0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a678b",
   "metadata": {},
   "source": [
    "**1.3 (4 pts): Compute the mean, median of the `Price` column.**\n",
    "\n",
    "1. Compute the mean and median of the `Price` column. If you encounter error, try to understand why this error is happening and propose a solution.\n",
    "2. After computing the mean and median, do you think they are reasonable? If not, what could be the reason for this? \n",
    "  \n",
    "Hint: Is all values in the `Price` column numerical?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c845f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be3a32",
   "metadata": {},
   "source": [
    "**1.4 (4 pts): Convert Units**\n",
    "\n",
    "From now on, we will work with the cleaned data `Task1-2.ToyotaCorolla-clean.csv`. Read it into a DataFrame.\n",
    "\n",
    "You notice that some prices are in CHF (Swiss Francs), while others are in EUR (Euros) or GBP (British Pounds). Additionally, for cars priced in GBP, the mileage is in miles rather than kilometers.\n",
    "\n",
    "For consistency, convert all prices to CHF and all distances to kilometers.\n",
    "\n",
    "- Exchange rates:\n",
    "  - 1 CHF = 1.05 EUR\n",
    "  - 1 GBP = 1.15 CHF\n",
    "  - 1 mile = 1.61 km\n",
    "\n",
    "Make the following conversions:\n",
    "1. Convert prices in EUR or GBP to CHF, rounding to the nearest integer.\n",
    "2. Convert distances in miles (for GBP cars) to kilometers, rounding to the nearest integer.\n",
    "3. Drop the 'Currency' column.\n",
    "4. Calculate the min, mean, median and max of the 'Price' and 'Distance' columns after the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b49ba1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48a107",
   "metadata": {},
   "source": [
    "**1.5 (2 pts): Analyze Average Price**\n",
    "\n",
    "A.  Print the average price for each fuel type. Determine which fuel type has the highest average price.\n",
    "\n",
    "B.  Print the average price for different numbers of doors. Determine which number of doors has the highest average price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91a42eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0085fbeb",
   "metadata": {},
   "source": [
    "**1.6 (2 pts): Relationship Between Car Age and Price**\n",
    "\n",
    "It is intuitive that an older car tends to be cheaper, and a car with more mileage might also be less expensive. \n",
    "\n",
    "To explore this intuition, create two scatter plots:\n",
    "1. Car Age vs Price\n",
    "2. Mileage vs Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d72d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35a238",
   "metadata": {},
   "source": [
    "**1.7 (4 pts): Correlation Between Price and Mileage**\n",
    "\n",
    "The relationship between car price and mileage appears non-linear, with a steeper price drop initially followed by a flatter curve.\n",
    "\n",
    "A.(2 pts)  Calculate both the Pearson and Spearman correlations between the price of the car and the distance driven.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b5927e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a201c",
   "metadata": {},
   "source": [
    "\n",
    "B.(2 pts)  Which correlation value is higher? Does this result align with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4272cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40229147",
   "metadata": {},
   "source": [
    "## Part 2 Linear Regression (30 pts)\n",
    "\n",
    "You want to build a linear regression model to predict the price of a car based on the features you have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc07fb",
   "metadata": {},
   "source": [
    "**2.0 (8 pts) Helper functions**\n",
    "\n",
    "Before building machine learning models, how to asses performance is crucial. Hence we first implement some helper functions to asses the performance of our model. We can use these later throughout the exercise.\n",
    "\n",
    "Implement the `accuracy` , `precision`, `recall` and `f1_score` functions with the following requirements:\n",
    "\n",
    "1. These functions should take in the true labels(`np.array`) and the predicted labels(`np.array`) and return the corresponding metric. \n",
    "2. They should follow the convention that the positive class is 1 and the negative class is 0.\n",
    "3. Apply the functions to the following data:\n",
    "\n",
    "```python\n",
    "true_labels = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
    "predicted_labels = np.array([1, 1, 1, 1, 0, 0, 1, 0, 1, 0])\n",
    "```\n",
    "\n",
    "Compare the results with the implementation in `sklearn` and see if they match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73e77205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fc74b9",
   "metadata": {},
   "source": [
    "\n",
    "**2.1 (6 pts) Preprocess the Data**\n",
    "\n",
    "To prepare your data for building a linear regression model, complete the following steps:\n",
    "\n",
    "A.(1 pts) Convert the categorical variables to one-hot encoding using the `pd.get_dummies()` function, how many columns do you have after the one-hot encoding? (P.S. You may want to avoid introducing multicollinearity with one-hot encoding, what should you do to avoid this?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c439e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977dc719",
   "metadata": {},
   "source": [
    "B.(1 pts) Split the data into features (X) and target (y) variables. The target variable is the 'Price' column. Then split the data into train test sets using a 80-20 split. Use `random_state=42` for reproducibility. How many samples are in the training set and how many samples are in the test set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "247139a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b674f",
   "metadata": {},
   "source": [
    "C.(1 pts) Why do we split the data into only train-test sets but not train-validation-test sets? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3380bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b840ea36",
   "metadata": {},
   "source": [
    "D.(1 pts) **Standardize the Features**: Use `StandardScaler` from `sklearn.preprocessing` and then add a constant column using `sm.add_constant()`. Print the average and standard deviation of the training set after standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cab9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cdffba",
   "metadata": {},
   "source": [
    "E.(2 pts) Should we first standardize the data and then split it into train and test sets or vice versa?   why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a691c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1efda85",
   "metadata": {},
   "source": [
    "**2.2 (10 pts) Train and Evaluate the Linear Regression Model**\n",
    "\n",
    "To train and evaluate a linear regression model using the `statsmodels` library, complete the following steps:\n",
    "\n",
    "1. (2 pts) Train a linear regression model on the training dataset using `sm.OLS` from `statsmodels`, print the summary of the model using `model.summary()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "647e254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f7a394",
   "metadata": {},
   "source": [
    "2. (2 pts) Evaluate the model on the test dataset using the square root of the mean squared error (RMSE) metric. \n",
    "   1. Report the RMSE value.\n",
    "   2. Your boss wants to know how far off the model's predictions are from the actual price of the car. What would you tell him? Given a number and explain how you got it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "763573c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2984353d",
   "metadata": {},
   "source": [
    "3. (2 pts) Report the R² score on the test dataset and interpret it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2f39c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc6757",
   "metadata": {},
   "source": [
    "4.  (2 pts) Which features are statistically significant at a 5% significance level? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9c47c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db567f",
   "metadata": {},
   "source": [
    "5.  (2 pts) Determine which two feature have the highest coefficient? What does it imply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3eba894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2d643",
   "metadata": {},
   "source": [
    "**2.3 (2 pts): Improvement Discussion**\n",
    "\n",
    "- Suggest a few additional features that could potentially explain this remaining variance in the data ( at least 2 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b371b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd829197",
   "metadata": {},
   "source": [
    "**2.4 (2 pts): Identifying Confounding Variables**\n",
    "\n",
    "The feature \"Weight\" shows a very low p-value and a high coefficient, but it doesn't seem to be a major factor for customers buying a second-hand car. You go to your mentor Tim to discuss this issue. Indeed, Tim suggests that never in his career has he seen a customer who asked for the weight of a car before buying it.\n",
    "You suspect that there might be a confounding variable that is correlated with the car's weight and significantly influences its price.\n",
    "\n",
    "- Suggest a possible confounding variable that may be correlated with the car's weight and significantly influence its price (it doesn't need to be a variable in the dataset). Explain why this variable could be a confounding variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2c258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104cb90e",
   "metadata": {},
   "source": [
    "**2.5 (2 pts): Adding an Inverse Mileage Term**\n",
    "\n",
    "From the previous scatter plot, the relationship between car price and mileage appears non-linear, with a steep price drop initially and then a flattening. A suitable approach to model this behavior is by incorporating an inverse term of mileage.\n",
    "\n",
    "- Add the inverse mileage term to the model and retrain it using the code provided. Print the model summary and interpret the effect of the inverse mileage term.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bca4a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ffbc4",
   "metadata": {},
   "source": [
    "## Part 3 Supervised Learning (40 pts)\n",
    "\n",
    "\n",
    "After completing your analysis, you're satisfied with the results. You handed the Jupyter notebook over to your mentor.\n",
    "\n",
    "(Fun fact: The name \"Jupyter\" is derived from Julia, Python, and R—three programming languages that the platform was originally designed for.)\n",
    "\n",
    "Your mentor Tim is very impressed with your work and asks you the following question:\n",
    "\n",
    "“\n",
    "This looks great! It will be very useful for our sales team. While looking at the results, I realized that there might be one thing that we can improve. \n",
    "For companies like us, it is important to sell the cars quickly. If we are patient, we might be able to sell the car for a higher price, but that’s not always the best strategy. We need to consider the maintenance costs for the car, the cash flow and the fact that the price of the car decreases over time.\"\n",
    "\n",
    "He then continues:\n",
    "\"Three months is a sweet spot for us. If we can sell the car within the first three months, it is great. If not, it is worth considering lowering the price to sell it faster and increase our cash flow. I can ask Ivan from Sales to collect data in the last few months on whether the car was sold within the first three months or not. This would be great if you could have a model that tells us if the car will be sold in the first three months or not. \"\n",
    "\n",
    "This sparks your interest, and soon Ivan has provided you with the new data containing an additional column `sold_within_3_months` which is a binary variable indicating whether the car was sold within the first three months or not.\n",
    "\n",
    "Note: The data for this part is in the file `Task3.ToyotaCorolla_sales_3months.csv` and it has already unified the currency and distance units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4ec9ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Price  Age     KM FuelType  HP  MetColor  Automatic    CC  Doors  Weight  \\\n",
      "0  13500   23  46986   Diesel  90         1          0  2000      3    1165   \n",
      "1  13750   23  72937   Diesel  90         1          0  2000      3    1165   \n",
      "2  13950   24  41711   Diesel  90         1          0  2000      3    1165   \n",
      "3  14950   26  48000   Diesel  90         0          0  2000      3    1165   \n",
      "4  13750   30  38500   Diesel  90         0          0  2000      3    1170   \n",
      "\n",
      "   sold_within_3_months  \n",
      "0                     0  \n",
      "1                     0  \n",
      "2                     0  \n",
      "3                     0  \n",
      "4                     0  \n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('data/Task3.ToyotaCorolla_sales_3months.csv', index_col=0)\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca7119",
   "metadata": {},
   "source": [
    "\n",
    "**3.1 (2 pts): Preprocess the Data**\n",
    "1. （1 pts）How many cars in the dataset were sold in the first three months, and how many were not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "214abae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d3b6f8",
   "metadata": {},
   "source": [
    "2. (1 pts) Preprocess the categorical variables to one-hot encoding using the `pd.get_dummies()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e2b1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289da79",
   "metadata": {},
   "source": [
    "**3.2 (20 pts): Logistic Regression Model**\n",
    "\n",
    "1. (2 pts) Split the data into features (X) and target (y) variables. The target variable is the 'sold_within_3_months' column. The `Price` column should be included as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "119c754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51f8664",
   "metadata": {},
   "source": [
    "2. (2 pts) Then split the data into train test sets using a 80-20 split. Use `random_state=42` for reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bcdb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32caf5d5",
   "metadata": {},
   "source": [
    "3. (2 pts) Standardize the features using `StandardScaler` from `sklearn.preprocessing` and then add a constant column using `sm.add_constant()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "587a4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041fe3b8",
   "metadata": {},
   "source": [
    "4. (2 pts) Fit a logistic regression model on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1489059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b9728",
   "metadata": {},
   "source": [
    "5. (2 pts) Evaluate the model on the test dataset using the accuracy score metric. Report the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "681d70b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757fff3",
   "metadata": {},
   "source": [
    "6. (2 pts) Calculate the precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce8f5574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f156a",
   "metadata": {},
   "source": [
    "\n",
    "7. (2 pts) Suppose that your company is running short on cash flow and needs to sell the cars quickly. How should you adjust the threshold for the logistic regression model to ensure that the company can sell the cars as quickly as possible?\n",
    "    - A. Increase the threshold\n",
    "    - B. Decrease the threshold\n",
    "\n",
    "In a more general sense, how does the choice of threshold affect the precision and recall of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3619ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6276c5",
   "metadata": {},
   "source": [
    "8. (6 pts) Use binary search to find the optimal threshold that maximizes the F1-score. Implement a binary search algorithm to find the threshold that maximizes the f1-score of the logistic regression model on the training set. The search interval should be between 0 and 1, and the stopping criterion is 10 iterations.  What is the optimal threshold and what difference does the optimal threshold make in the F1-score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5545ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84e604",
   "metadata": {},
   "source": [
    "**3.3(18 pts) Decision Tree Model**\n",
    "\n",
    "Use a Decision Tree model from `sklearn` to predict whether a car will be sold within the first three months.\n",
    "\n",
    "Follow these steps to complete the task:\n",
    "\n",
    "1. (2 pts) Train a Decision Tree Classifier to predict the target variable (`sold_within_3_months`).You can reuse the train and test sets from the previous section. Set `random_state=42` for reproducibility in `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b25eec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63335f2",
   "metadata": {},
   "source": [
    "2. (2 pts) Evaluate the model on the test set and report the depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10cc6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4f6ff",
   "metadata": {},
   "source": [
    "3. (2 pts) Visualize the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80a4220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814fe768",
   "metadata": {},
   "source": [
    "4. (2 pts) Retrain the Decision Tree Classifier with a maximum depth of 8 and evaluate it on the test set. Compare and explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f68a02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5cf86",
   "metadata": {},
   "source": [
    "5. (6 pts) Train a Decision Tree Classifier for each depth from 1 to D where D is the maximum depth of the Decision Tree Classifier seen in the previous step. Evaluate each model on the test set and plot the accuracy of the models as a function of the depth and find the optimal depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a468a503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ddb52",
   "metadata": {},
   "source": [
    "6. (4 pts) Train a decision tree of depth = 1 , visualize the tree and explain what is the decision rule at the root node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "622339ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15234a8",
   "metadata": {},
   "source": [
    "## Part 4 Propensity Score Matching (10 pts)\n",
    "\n",
    "Your mentor is thrilled with the progress, and he has asked Ivan to put the model into production. Based on the model's prediction, the sales manager Ivan will decide whether to lower the car's price by 5%.\n",
    "\n",
    "A new quarter has passed, and Ivan has collected updated sales data, which includes the following columns:\n",
    "\n",
    "- `Price`: The initial price of the car.\n",
    "- `Pred_Prob`: The predicted probability of the car being sold within the first three months.\n",
    "- `Applied_Discount`: Whether the discount was applied (Yes=1, No=0).\n",
    "- `Discounted_Price`: The car's final price, calculated as `Price * 95%` if the discount was applied; otherwise, it's equal to `Price`.\n",
    "- `Sold_within_3_months`: Whether the car was sold within the first three months (Yes=1, No=0).\n",
    "\n",
    "Your task is to estimate the causal effect of the discount on sales within the first three months using propensity score matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "436d7945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Price  Pred_Prob  Applied_Discount  Discounted_Price  Sold_within_3_months\n",
      "0  12750       0.01                 1           11475.0                     1\n",
      "1  21950       0.00                 1           19755.0                     1\n",
      "2   9950       0.79                 0            9950.0                     1\n",
      "3   9930       0.91                 1            8937.0                     0\n",
      "4   9450       0.97                 0            9450.0                     0\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('data/Task4.ToyotaCorolla_discount_sales.csv', index_col=0)\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd889819",
   "metadata": {},
   "source": [
    "**4.1 (1 pts): How many samples are in the treated group, and how many are in the control group?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe5f7e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c42cde",
   "metadata": {},
   "source": [
    "\n",
    "**4.2 (5 pts): Propensity Score Matching**\n",
    "- The propensity score is the predicted probability of the car being sold within the first three months from the logistic regression model, i.e. `Pred_Prob` column in the `Task4.ToyotaCorolla_discount_sales.csv` file. Create pairs of matched samples as follows:\n",
    "  - For each treated sample (discount applied), find a control sample (discount not applied) with a difference in propensity score of less than 0.05.\n",
    "  - If there is more than one control sample for a treated sample, choose the control sample with the smallest difference in propensity score.\n",
    "  - If there is no control sample satisfying the condition, discard the treated sample.\n",
    "  - How many successful matches did you get?\n",
    "\n",
    "Notice that your output should be 1-to-1 matching, meaning that each treated sample should be matched with at most one control sample.\n",
    "So each sample can only appear once in the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7dbadaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b061ed",
   "metadata": {},
   "source": [
    "**4.3 (4 pts): Average Treatment Effect (ATE)**\n",
    "\n",
    "Now let's estimate the effect of the discount on sales. \n",
    "\n",
    "For each matched pair, there is one treated sample and one control sample. They may have different outcomes and we can calculate the average treatment effect (ATE) as \n",
    "\n",
    "$$ ATE = \\frac{1}{N} \\sum_i^N  y_{treat}^{(i)} - y_{\\\\control}^{(i)} $$ \n",
    "\n",
    "where $y_{treat}^{(i)}$ and $y_{\\\\control}^{(i)}$ are the outcomes for the treated and control samples, respectively.\n",
    "\n",
    "Notice that here the outcome is a simple binary variable, which is whether the car was sold within the first three months or not.\n",
    "\n",
    "1. (3 pts) Calculate the ATE based on the matched pairs and report the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3a6d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547643f0",
   "metadata": {},
   "source": [
    "2. (1 pts) What is your conclusion about the effect of the discount on sales within the first three months?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "560573b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
